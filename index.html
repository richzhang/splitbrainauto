<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1000px;
	}	
	h1 {
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 0px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
  <head>
		<title>Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction</title>
		<meta property="og:image" content="./index_files/arxiv2016_splitbrain_v2.png"/>
		<meta property="og:title" content="Split-Brain Autoencoders: Unsupervised Learning for Cross-Channel Prediction. Zhang, Isola, Efros. In CVPR, 2017." />
  </head>

  <body>
    <br>
          <center>
          	<!-- <span style="font-size:42px">Split-Brain Autoencoders:</span><br> -->
          	<!-- <span style="font-size:32px">Unsupervised Learning by Cross-Channel Prediction</span> -->
          	<span style="font-size:42px">Split-Brain Autoencoders:</span><br>
          	<span style="font-size:42px">Unsupervised Learning by Cross-Channel Prediction</span><br>
	  		  <table align=center width=600px>
	  			  <tr>
	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:24px"><a href="https://richzhang.github.io/">Richard Zhang</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:24px"><a href="http://web.mit.edu/phillipi/">Phillip Isola</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:24px"><a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a></span>
		  		  		</center>
		  		  	  </td>
			  </table>
          	<span style="font-size:22px">Department of EECS, University of California, Berkeley</span>
          	<!-- <span style="font-size:22px">In CVPR, 2017.</span><br> -->
          		<!-- <span style="font-size:30px">ECCV 2016.</span> -->

	  		  <table align=center width=400px>
	  			  <tr>
	  	              <td align=center width=300px>
	  					<center>
	  						<span style="font-size:22px"><a href='https://github.com/richzhang/splitbrainauto'> Code [GitHub]</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=300px>
	  					<center>
	  						<span style="font-size:22px"><a href='https://arxiv.org/abs/1611.09842'> CVPR 2017 [Paper]</a></span>
		  		  		</center>
		  		  	  </td>
			  </table>
          </center>
  		  <br>

  		  <table align=center width=800px>
  			  <tr>
  	              <td width=400px>
  					<center>
  	                	<img class="rounded" src = "./index_files/arxiv2016_splitbrain_v2.png" height="275px"></img>
  	                	<br>
					</center>
  	              </td>
                </tr>
  	              <td width=400px>
  					<center>
  	                	<!-- <span style="font-size:14px"><i>Example  input  grayscale  photos  and  output  colorizations  from  our  algorithm. These examples are cases where our model works especially well. For randomly selected examples, see the <a href="#perform_comp"><b>Performance comparisons</b></a> section below.</i> -->
					</center>
  	              </td>
  		  </table>

  		  <br>
		  <hr>

  		  <table align=center width=900px>
	  		  <center><h1>Abstract</h1></center>
					We propose split-brain autoencoders, a straightforward modification of the traditional autoencoder architecture, for unsupervised representation learning. The method adds a split to the network, resulting in two disjoint sub-networks. Each sub-network is trained to perform a difficult task -- predicting one subset of the data channels from another. Together, the sub-networks extract features from the entire input signal. By forcing the network to solve cross-channel prediction tasks, we induce a representation within the network which transfers well to other, unseen tasks. This method achieves state-of-the-art performance on several large-scale transfer learning benchmarks.
  		  <br>
		  <hr>

 		<center><h1>Method</h1></center>
	  		To perform unsupervised pretraining, we split a network in half in the channel direction to produce two disjoint subnetworks. Each subnetwork is then trained to perform prediction on one subset of data from another subset. <b>(Left) Images</b> Half of the network predicts color channels from grayscale, and the other half predicts grayscale from color. <b>(Right) RGB-D Images</b> Half of the network predicts depth from images, and the other half predicts images from depth. Please see Section 3 of the full <a href='https://arxiv.org/abs/1611.09842'>paper</a> for additional details.

  		  <table align=center width=900px>
	  		<br>
  			  <tr>
  	              <td align=center width=400px>
						  <td><img class="round" style="width:400px" src="./index_files/imgs/fig2a.png"/></a>
						  </td>
						  <td width=100px></td>
				  </td>
  	              <td align=center width=400px>
						  <td><img class="round" style="width:400px" src="./index_files/imgs/fig2b.png"/></a>
						  </td>
	  		  	  </td>
			  </tr>

 		</table>

		  <br>
		  We show that we can learn features in an unsupervised framework, simply by predicting <i>channels of raw data</i> from other channels of raw data.
		  <hr>

 		<center><h1>Feature Evaluation Results</h1></center>
  		Here, we show feature evaluation results on large-scale RGB images, which are described in Section 4.1 of the <a href='https://arxiv.org/abs/1611.09842'>paper</a>. Results on the RGB-D domain are in Section 4.2.

  		The first group of tests are linear classifiers for semantic classification on each layer in the network. We freeze pre-trained AlexNet representations, spatially resize feature maps so that all layers have approximately 9000 dimensions, and train multinomial logistic regression classifiers on the <b>(Left) ImageNet</b> and <b>(Right) Places</b> datasets. <b>ImageNet-labels</b> and <b>Places-labels</b> are networks which are pre-trained in a supervised regime. All other methods shown are unsupervised. Method <b>Split-Brain Auto (cl,cl)</b> is our proposed method, and performs well compared to previous unsupervised/self-supervised methods.

  		<table align=center width=800px>
  		<br>
  		<br>
  			  <!-- <tr> -->
  	              <td align=center width=800px>
  					<center>
						  <td><img class="round" style="width:500px" src="./index_files/imgs/ilsvrclin_comp_methods.png"/></a>
						  </td>
						  <td><img class="round" style="width:500px" src="./index_files/imgs/placeslin_comp.png"/></a>
						  </td>
	  		  		</center>
	  		  		</td>
			  <!-- </tr> -->
		  </table>

		<br>
		We also show performance on several commonly used transfer learning benchmarks on the PASCAL VOC dataset. The table below is Table 4 from the <a href='https://arxiv.org/abs/1611.09842'>paper</a>, and additional details can be found in Section 4.1.1.

  		<table align=center width=500px>
  		<br>
  		<br>
  	              <td align=center width=500px>
  					<center>
						  <td><img class="round" style="width:500px" src="./index_files/imgs/pascal_v2.png"/></a>
						  </td>
	  		  		</center>
	  		  		</td>
		  </table>

      	  <br>
		  <hr>

  		  <table align=center width=575px>
	 		<center><h1>Paper</h1></center>
  			  <tr>
				  <td><a href="https://arxiv.org/abs/1611.09842"><img class="layered-paper-big" style="height:175px" src="./index_files/imgs/page1.png"/></a></td>
				  <td><span style="font-size:14pt">R. Zhang, P. Isola, A. A. Efros.<br>
				  <b>Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction.</b><br>
				  In CVPR, 2017. (hosted on <a href="https://arxiv.org/abs/1611.09842">arXiv</a>)</a>
				  <span style="font-size:4pt"><a href=""><br></a>
				  </span>
				  </td>
  	              </td>
              </tr>
  		  </table>
		  <br>

		  <table align=center width=600px>
			  <tr>
				  <td><span style="font-size:24pt"><center>
				  	<a href="./index_files/bibtex_cvpr2017_splitbrain.txt">[Bibtex]</a>
  	              </center></td>
              </tr>
  		  </table>

  		  <br>
		  <hr>

 		<center><h1>Try the model</h1></center>
		  The GitHub page has scripts for fetching our model, and a slightly modified version of Caffe which does color pre-processing (if so desired). We recommend going to the <a href='https://github.com/richzhang/splitbrainauto'>GitHub</a> page and following instructions in the readme.

  		  	  <br>

  		  <table align=center width=600px>
  			  <tr>
  			  <br>
  	                <td align=center width=600px>
  					<center>
						  <td><a href='https://github.com/richzhang/splitbrainauto'><img class="round" style="width:600px" src="./index_files/imgs/alexnet.png"/></a></td>
	  		  		</center>
	  		  		</td>
			  </tr>
		  </table>

  		  <table align=center width=800px>
			  <tr><center>
				<span style="font-size:28px"><a href='https://github.com/richzhang/splitbrainauto'>[GitHub]</a>
			  <br>
			  </center></tr>
			  <!-- <tr><center> -->
 			<!-- <span style="font-size:28px"><a>&nbsp;Model: [Prototxt] Weights: [Unrescaled] [Rescaled]</a></span> -->
			  <!-- </center></tr> -->
		  </table>


  		  <br>
		  <hr>
		  	
  		  <table align=center width=1000px>
  			  <tr>
  	              <td width=400px>
  					<left>
	  		  <center><h1>Acknowledgements</h1></center>
	  		  We thank members of the Berkeley Artificial Intelligence Research Lab (BAIR), in particular Andrew Owens, for helpful discussions, as well as Saurabh Gupta for help with RGB-D experiments. This research was supported, in part, by Berkeley Deep Drive (BDD) sponsors, hardware donations by NVIDIA Corp and Algorithmia, an Intel research grant, NGA NURI, and NSF SMA-1514512. Thanks Obama.
			</left>
		</td>
			 </tr>
		</table>

		<br><br>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75863369-2', 'auto');
  ga('send', 'pageview');

</script>
              
</body>
</html>
 
